# 特定物体識別器をつくる (インスタンス識別)
## データセットを作る
自作データからデータセットを作成する．
### 撮影
iPhoneなどを用いて対象物を撮影する．

後の処理のために，あまり対象物のスケールが変化しないようにカメラと対象物の距離が変わらないように撮影する．

手ブレがあまり発生しないようにする．

見えの変化の影響を検証するために，色んな角度から撮影する．また，色んな日照状況下で撮影する．

解像度は統一しなくても大丈夫

### ディレクトリ構造
撮影した動画をローカルPCに移し，以下のようなディレクトリ構造を作成する．
~~~
dataset/
 + instanceA/
    + instanceA_video1.mp4
    + instanceA_video2.mp4
 + instanceB/
    + instanceB_video1.mp4
    + instanceB_video2.mov
~~~
* 対象物(インスタンス)ごとにディレクトリを作成し，その中に対象物が映っている動画を入れる．

### 動画を画像にサンプリングする
**ffmpegをinstallしておく**．たぶんHomebrewとかapt-getとかでinstallできます．
~~~
$ video2image.py <target_dataset_dir>
~~~
* <target_dataset_dir>に指定したディレクトリ以下の動画すべてを，画像にサンプリングします(再帰的に探索します)
* 動画名と同名のディレクトリが作成され，そこにサンプリングした画像が保存されます．
* 動画をあとから追加する場合も，ディレクトリに追加したあとにこのコードをそのまま実行して大丈夫です．
* 動画の拡張子は.mp4, .mov, .MOV, m4vなど．
* 他の拡張子を用いる場合は，[video2image.py](https://github.com/shigenius/tensorflow_works/blob/master/tf-slim/script/video2image.py)のどこかに記述してある`pattern = r"\.(mp4|m4v|mov|MOV)$"`に拡張子を追加してください．

サンプリングする際のスキップレート(デフォルトは5)を変更する場合は，同じくコード内の
~~~
subprocess.check_call(['ffmpeg', '-i',
                       target,
                       '-r', '5',
                       os.path.join(save_dir, 'image_%04d.jpg')])
~~~
の'-r', '5',の部分を変更してください．
### Trackingツールを用いて半自動で対象物体のcrop画像をつくる．
3通りの作り方があります．

1. [opencv_tracking.py](https://github.com/shigenius/python_sources)
~~~
% python opencv_tracking.py -s <skip_rate> -x <rect_width> -y <rect_height> <movie_path> <save_dir_path>
~~~
* 物体のトラッキング+トラッキングしている点を中心として矩形を抜き出す．
* 実行後，最初のフレームでマウスクリックしたところに一番近い特徴点をトラッキングする．(Rキーでrun，Sキーでstop，spaceキーで1f進む)
* 実行中でも新しくクリックした箇所の特徴点を追跡するように更新されます．
* [上](#動画を画像にサンプリングする)でskip-rateを5でサンプリングしている場合は，`-s 5`としてください．
* 30fpsの動画のみ対応. 他はおかしくなります．
* skip-frameの処理がffmpegと違うため，frameのindexが正しく一致しない問題がある．(ほんの数フレームだけだが)
2. [cropRect_with_FeatureTracking.py](https://github.com/shigenius/tensorflow_works/blob/master/tf-slim/script/cropRect_with_FeatureTracking.py) (オプティカルフロー)
~~~
% python cropRect_with_FeatureTracking.py --algo 'o' -s <skip_rate> -W <rect_width> -H <rect_height>  <target_dir_path> <save_dir_path>
~~~
* 上と同じアルゴリズムだが，動画ではなく画像の入ったディレクトリを入力にできる点が違う．
* このコード上でのskip frameは画像を何枚飛ばすかなので，[サンプリングする際](#動画を画像にサンプリングする)にskip rateを0にしておくと良い．
3. [cropRect_with_FeatureTracking.py](https://github.com/shigenius/tensorflow_works/blob/master/tf-slim/script/cropRect_with_FeatureTracking.py) (KCF)
~~~
% python cropRect_with_FeatureTracking.py --algo 'k' -s <skip_rate> -W <rect_width> -H <rect_height>  <target_dir_path> <save_dir_path>
~~~
* KFCでTrackingします．
* 最初のフレームでマウスでドラッグしてrectの範囲を指定できます．
* crop画像のアス比を揃えたい場合は1, 2の作り方をした方がいいかもです．


いずれの作り方でも処理ごとにsubwindow_log.txtが作成されます．(cropしたrectangleの情報が格納してある)

画像が大きすぎてクリックできない場合は，[resize_all_images_by_half.py](https://github.com/shigenius/tensorflow_works/blob/master/tf-slim/script/resize_all_images_by_half.py)を使ってリサイズするといいです．(元画像が破壊されるので注意)

### Data Augmentation
~~~
% python data_augmentation.py -dataset <dataset_path> -r <extension_rate>
~~~
* 指定したdatasetをaugmentして各ディレクトリに保存する．
* `-r`オプションでデータを何倍にするか指定できます．
* augmentationの詳細:
  - random contrast
  - gaussian noise
  - random brightness
  - random_rotate
  - 平行移動
* すべて標準正規乱数に従います．(元画像に近いデータが作成されやすい)

### データセットをTrain, Valid, Testに分ける
二通りあります．

1.[日照状況を考慮した分け方](https://github.com/shigenius/tensorflow_works/blob/master/tf-slim/script/makeDataset_forFIT.py)
~~~
% python makeDataset_forFIT.py <dataset_path>
~~~
* 実行すると，trainとvalidに割り当てる日照状況クラスタの番号入力が求められます．
* trainに指定した一つの日照状況クラスタ，validにもうひとつの日照状況クラスタ，testには残りのすべてのデータが割り当てられます．
* trainのみaugmentしたデータを含みます．
* 日照状況クラスタとは，動画の撮影時刻から1時間前後に撮影した動画をまとめたものです．

2.[ランダムで分ける](https://github.com/shigenius/tensorflow_works/blob/master/makeDataset_forSpecificObjRecog.py)
~~~
% python makeDataset_forSpecificObjRecog.py <dataset_path>
~~~
* trainのみaugmentしたデータを含みます．
* 少し書き方がダメなので実行が終わるまでとても時間が掛かります．
## 学習
作成したデータセットを用いてインスタンス識別器をつくる．

### 転移学習用のpre-trained modelを持ってくる．
[tensorflow/models/research/slim/](https://github.com/tensorflow/models/tree/master/research/slim)
からckptファイルをダウンロードする．
### 学習
リモートサーバー側でscreenコマンドを使っておくと，sshセッションが切れてもpythonのプロセスがkillされないので便利．

*CBO-Net*
~~~
% python transfer_learning.py --train_c <train_crop.txt> --test_c <valid_crop.txt> --train_o <train_orig.txt> --test_o <valid_orig.txt> --model_path <pre-trained_.ckpt> -extractor <'vgg_16'|'inception_v4'> -save <save_file.ckpt> --summary_dir <save_suppary_dir_path> -b <batch_size> -s <num_step> -ns <num_of_classes (SpecificRecog)> -ng <num_of_classes (GeneralRecog)>
~~~

*VGG16 (or inception-V4)*
~~~
% python comparison_learning.py --train_c <train_crop.txt> --test_c <valid_crop.txt> --train_o <train_orig.txt> --test_o <valid_orig.txt> --model_path <pre-trained_.ckpt> -save <ckpt_path_to_store> --summary_dir <save_suppary_dir_path> -b <batch_size> -s <num_step> -nc <num_of_classes>
~~~
* 本来は--train_oと--test_oは要らないですが，Datasetモジュールを使いまわしているため必要になっています．あとその分動作が重たくなっています．



学習の進捗状況はTensorBoardを使って見ることができます．
~~~
 # ローカル側から
% ssh -L 8888:localhost:6006 <userid@address>
 # リモート側に接続したら
% tensorboard --logdir <log_dir>
~~~
これでローカル環境のブラウザから`http://localhost:8888/`でTensorboardを開くことができます．

## 評価
先にリモート側でevaluationしたあとにローカル側で結果を見ます．

*リモート側で*
~~~
% python eval.py -c <test_crop.txt> -o <test_orig.txt> -net <'shigeNet'|'vgg_16''> -extractor <'vgg_16'|'inception_v4'> --num_classes <num_classes> --restore_path <restore_model_path> -log <hoge.csv>
~~~
* testデータをすべて評価し，結果を指定したパスのcsvファイルに記述します．


**scpとかを使ってローカル環境にcsvファイルを持ってくる**

1.照明状況を考慮した場合

*ローカル側で*
~~~
% python getAcc_for_luminous_condition.py <dataset_path> <target_csv_file_path> <label_path>
~~~
* 照明クラスタ毎にtest accuracyを算出します．(ちょっと見辛いです．)あとmean accuracyも算出します．

2.混合行列を算出する．
~~~
% python makeConfusionMatrix.py <target_csv_file_path> <label_path>
~~~
3.識別に失敗している画像をみる
~~~
% python cp_failed_image.py <target_csv_file_path>  <save_dir_path>
~~~

## 注意すること
* コマンドミス
* Train時とTest時のlabelのidが変わっていないか
  - 変わっている場合は[script/replace_labelid.py](https://github.com/shigenius/tensorflow_works/blob/master/tf-slim/script/replace_labelid.py)でlabelidを置き換えることができます．

  # 一般物体認識器をつくる
  ## データセット
  ### imagenetから一般物体の画像をダウンロードする．
  ~~~
  % python collectImagenet.py <output_dir> --num_of_classes 10 --num_of_pics 1000 --offset_class 0 --offset 0
  ~~~
  * python2系
  * 36行目にcategoryの名前とidの辞書を記述している．
  * 40~42 行目のコメントアウトを外せば，imagenetのすべてのカテゴリからランダムで取ってくるようになる．
  * オプションの --offset_class  --offset はダウンロードを途中から始めたいときに用いる．
    - それぞれ何番目のクラス，枚数から始めるかint型で入力
    - 毎回urlをシャッフルしているので画像が重複する可能性がある．

  ### 一般物体認識器の学習用データセットを作成する．
  ~~~
  % makeDataset_forimagenet.py <imagenet_images_dir_path> <specific_object_dataset_path> -n <target_name>
  ~~~
  * Specific object recognitionデータセットを作成していることを前提とする．(詳しくは[makeDataset_forFIT.py](https://github.com/shigenius/tensorflow_works#makeDataset_forFIT.py))
  * 対象物体が複数ある場合は，<specific_object_dataset_path>に以下のformatで記述したtxtファイルを指定する．-nオプションは指定しなくてよい
  ~~~
  <dataset1_path> <dataset1_name>
  <dataset2_path> <dataset2_name>
  ~~~
  * negativeデータは<specific_object_dataset_path>下からコピーするが，存在しない場合は[makeRandomCropping_NegativeData.py](https://github.com/shigenius/tensorflow_works#makeRandomCropping_NegativeData.py)を用いて<imagenet_images_dir_path>下にnegativeサンプルを作成する．

  ## 学習
  あとでかく
